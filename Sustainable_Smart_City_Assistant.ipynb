{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289e17aa",
   "metadata": {},
   "source": [
    "# üèôÔ∏è Sustainable Smart City Assistant\n",
    "\n",
    "A comprehensive AI-powered assistant for smart city management with the following features:\n",
    "- üìã Policy Analysis\n",
    "- üì¢ Citizen Reports\n",
    "- üìà KPI Forecasting\n",
    "- üå± Eco Tips\n",
    "- üîç Anomaly Detection\n",
    "- üí¨ AI Chat Assistant\n",
    "- üöó Traffic Route Suggestions\n",
    "\n",
    "## ‚öôÔ∏è Setup Instructions:\n",
    "1. Run all cells in order\n",
    "2. Update your API tokens in the configuration cell\n",
    "3. The system will create a public URL using ngrok for external access\n",
    "4. Use the generated frontend interface to interact with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9ba6d",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages - optimized for Google Colab with IBM Granite support\n",
    "!pip install transformers torch pandas numpy scikit-learn gradio requests pyngrok huggingface_hub accelerate\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "import gradio as gr\n",
    "from IPython.display import display, HTML\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device availability and set appropriate configurations\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"üöÄ GPU detected - IBM Granite model will run faster!\")\n",
    "else:\n",
    "    print(\"Running on CPU - this is normal for Google Colab free tier\")\n",
    "    print(\"‚ö†Ô∏è IBM Granite model will load slower on CPU but will still work\")\n",
    "\n",
    "print(\"‚úÖ Dependencies installed and libraries imported successfully!\")\n",
    "print(\"ü§ñ Ready for IBM Granite model initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17b4a9",
   "metadata": {},
   "source": [
    "## üîë Configuration - Update Your API Keys Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Replace these with your actual API tokens\n",
    "# Get Hugging Face token from: https://huggingface.co/settings/tokens\n",
    "# Get ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "# REQUIRED: HuggingFace Token for IBM Granite Model Access\n",
    "HF_TOKEN = \"token\"  # Replace with your Hugging Face token\n",
    "\n",
    "# OPTIONAL: ngrok Token for Public Access\n",
    "NGROK_TOKEN = \"2ypRPxyDYuES00zDxcQN1J3B9hr_5PzCGfzCAFDg7wE5j27Fd\"  # Replace with your ngrok token\n",
    "\n",
    "print(\"üîë Configuration loaded!\")\n",
    "print(f\"ü§ó HF Token configured: {'‚úÖ' if HF_TOKEN != 'token' else '‚ùå Please update HF_TOKEN'}\")\n",
    "print(f\"üåê ngrok Token configured: {'‚úÖ' if NGROK_TOKEN != 'token' else '‚ùå Please update NGROK_TOKEN'}\")\n",
    "\n",
    "# Validate HuggingFace token if provided\n",
    "if HF_TOKEN != \"token\":\n",
    "    try:\n",
    "        # Test HuggingFace authentication\n",
    "        from huggingface_hub import whoami\n",
    "        user_info = whoami(token=HF_TOKEN)\n",
    "        print(f\"‚úÖ HuggingFace authentication successful! User: {user_info['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå HuggingFace authentication failed: {e}\")\n",
    "        print(\"Please check your token at: https://huggingface.co/settings/tokens\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è HuggingFace token is required for IBM Granite model!\")\n",
    "    print(\"1. Go to: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Create a new token with 'Read' permissions\")\n",
    "    print(\"3. Replace 'token' above with your actual token\")\n",
    "    print(\"4. You may also need to request access to IBM Granite models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514988f9",
   "metadata": {},
   "source": [
    "## ü§ñ Smart City Assistant Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartCityAssistant:\n",
    "    def __init__(self, hf_token):\n",
    "        \"\"\"Initialize the Smart City Assistant with IBM Granite model\"\"\"\n",
    "        self.hf_token = hf_token\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # IBM Granite model configuration\n",
    "        self.model_name = \"ibm-granite/granite-3.0-2b-instruct\"  # Updated model name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.generator = None\n",
    "\n",
    "        # Storage for reports and data\n",
    "        self.citizen_reports = []\n",
    "        self.kpi_data = {}\n",
    "\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load IBM Granite model with proper authentication\"\"\"\n",
    "        if not self.hf_token or self.hf_token == \"token\":\n",
    "            print(\"‚ùå HuggingFace token is required for IBM Granite model!\")\n",
    "            print(\"Please update your HF_TOKEN in the configuration cell.\")\n",
    "            print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            print(\"üöÄ Loading IBM Granite model...\")\n",
    "            print(f\"Model: {self.model_name}\")\n",
    "            \n",
    "            # Load tokenizer first\n",
    "            print(\"üì• Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Add pad token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"üì• Loading model...\")\n",
    "            # Load model with appropriate settings for Colab\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Move model to device if not using device_map\n",
    "            if not torch.cuda.is_available():\n",
    "                self.model = self.model.to(self.device)\n",
    "            \n",
    "            print(\"üîß Creating text generation pipeline...\")\n",
    "            # Create pipeline\n",
    "            self.generator = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else -1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=512,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ IBM Granite model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading IBM Granite model: {e}\")\n",
    "            print(\"\\nTroubleshooting steps:\")\n",
    "            print(\"1. Verify your HuggingFace token is correct\")\n",
    "            print(\"2. Check if you have access to the IBM Granite model\")\n",
    "            print(\"3. Try running: !huggingface-cli login\")\n",
    "            \n",
    "            # Set generator to None to use fallback responses\n",
    "            self.generator = None\n",
    "            print(\"üîÑ Will use rule-based responses as fallback\")\n",
    "\n",
    "    def generate_response(self, prompt, max_tokens=300):\n",
    "        \"\"\"Generate response using IBM Granite model\"\"\"\n",
    "        if self.generator is None:\n",
    "            print(\"‚ö†Ô∏è Model not loaded, using fallback response\")\n",
    "            return self._fallback_response(prompt)\n",
    "            \n",
    "        try:\n",
    "            # Format prompt for IBM Granite (instruction format)\n",
    "            formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.generator(\n",
    "                formatted_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "                return_full_text=False  # Only return the generated part\n",
    "            )\n",
    "            \n",
    "            # Extract the generated text\n",
    "            generated_text = response[0]['generated_text'].strip()\n",
    "            \n",
    "            # Clean up the response\n",
    "            if \"<|assistant|>\" in generated_text:\n",
    "                generated_text = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "            \n",
    "            return generated_text if generated_text else self._fallback_response(prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error generating response: {e}\")\n",
    "            return self._fallback_response(prompt)\n",
    "\n",
    "    def _fallback_response(self, prompt):\n",
    "        \"\"\"Rule-based fallback responses when model is unavailable\"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        if 'policy' in prompt_lower or 'summarize' in prompt_lower:\n",
    "            return \"\"\"**Policy Summary:**\n",
    "This policy document contains important information for citizens:\n",
    "\n",
    "**Main Objectives:**\n",
    "- Improve city services and infrastructure\n",
    "- Enhance citizen engagement and participation\n",
    "- Promote sustainable development practices\n",
    "\n",
    "**Key Changes:**\n",
    "- New procedures for citizen feedback\n",
    "- Updated service delivery timelines\n",
    "- Enhanced transparency measures\n",
    "\n",
    "**Implementation Timeline:**\n",
    "- Phase 1: Immediate implementation of priority items\n",
    "- Phase 2: 6-12 months for major infrastructure changes\n",
    "- Phase 3: Long-term monitoring and evaluation\n",
    "\n",
    "For detailed information, please contact your local city administration.\"\"\"\n",
    "        \n",
    "        elif 'traffic' in prompt_lower or 'route' in prompt_lower:\n",
    "            return \"\"\"**Traffic Route Suggestions:**\n",
    "\n",
    "**Optimal Route Planning:**\n",
    "1. Use main arterial roads during off-peak hours (10 AM - 3 PM and 7 PM - 8 AM)\n",
    "2. Consider alternate routes during rush hours (7-10 AM, 4-7 PM)\n",
    "3. Check real-time traffic apps before departure\n",
    "\n",
    "**Popular Attractions Nearby:**\n",
    "- City center shopping districts\n",
    "- Cultural landmarks and museums\n",
    "- Public parks and recreational areas\n",
    "\n",
    "**Best Travel Times:**\n",
    "- Weekdays: Mid-morning or early afternoon\n",
    "- Weekends: Morning hours typically have lighter traffic\n",
    "\n",
    "**Transportation Options:**\n",
    "- Public transit: Bus and metro systems\n",
    "- Ride-sharing services\n",
    "- Bicycle lanes and walking paths\n",
    "- Park-and-ride facilities\"\"\"\n",
    "        \n",
    "        elif 'eco' in prompt_lower or 'environment' in prompt_lower or 'tips' in prompt_lower:\n",
    "            return \"\"\"**Eco-Friendly Tips for City Residents:**\n",
    "\n",
    "**Individual Actions:**\n",
    "1. **Energy Conservation:** Use LED bulbs, unplug devices when not in use, optimize heating/cooling\n",
    "2. **Water Conservation:** Fix leaks promptly, use low-flow fixtures, collect rainwater\n",
    "3. **Transportation:** Walk, bike, or use public transit when possible\n",
    "4. **Waste Reduction:** Practice the 3 R's - Reduce, Reuse, Recycle\n",
    "\n",
    "**Community Actions:**\n",
    "5. **Local Initiatives:** Join neighborhood clean-up events, support local farmers markets\n",
    "6. **Green Spaces:** Participate in tree planting and community garden projects\n",
    "7. **Advocacy:** Support environmentally-friendly city policies and initiatives\n",
    "\n",
    "These small changes can make a significant impact on our city's sustainability!\"\"\"\n",
    "        \n",
    "        elif 'anomaly' in prompt_lower or 'detect' in prompt_lower or 'data' in prompt_lower:\n",
    "            return \"\"\"**Anomaly Detection Analysis:**\n",
    "\n",
    "**Data Pattern Analysis:**\n",
    "The unusual patterns detected in the city data may indicate:\n",
    "\n",
    "**Possible Causes:**\n",
    "- Seasonal variations or weather-related impacts\n",
    "- Equipment malfunction or calibration issues\n",
    "- Unexpected events or policy changes\n",
    "- Data collection or transmission errors\n",
    "\n",
    "**Recommended Actions:**\n",
    "1. **Immediate:** Verify data collection equipment and processes\n",
    "2. **Short-term:** Investigate potential causes and correlate with city events\n",
    "3. **Long-term:** Implement monitoring systems to detect future anomalies\n",
    "\n",
    "**For City Administrators:**\n",
    "- Review maintenance schedules for monitoring equipment\n",
    "- Cross-reference with recent policy implementations\n",
    "- Consider environmental factors that may affect measurements\"\"\"\n",
    "        \n",
    "        elif 'chat' in prompt_lower or 'question' in prompt_lower or 'smart city' in prompt_lower:\n",
    "            return \"\"\"**Smart City Information:**\n",
    "\n",
    "Smart cities leverage technology and data to improve urban life through:\n",
    "\n",
    "**Key Benefits:**\n",
    "- Enhanced public services and infrastructure\n",
    "- Better resource management and sustainability\n",
    "- Improved citizen engagement and participation\n",
    "- Data-driven decision making for city planning\n",
    "\n",
    "**Core Components:**\n",
    "- IoT sensors for real-time monitoring\n",
    "- Digital platforms for citizen services\n",
    "- Sustainable transportation systems\n",
    "- Smart energy and water management\n",
    "\n",
    "**Citizen Involvement:**\n",
    "- Digital service portals and mobile apps\n",
    "- Community feedback and reporting systems\n",
    "- Participatory budgeting and planning\n",
    "- Environmental monitoring and awareness\n",
    "\n",
    "How can I help you learn more about specific smart city services?\"\"\"\n",
    "        \n",
    "        else:\n",
    "            return \"\"\"**Smart City Assistant Response:**\n",
    "\n",
    "Thank you for your inquiry about smart city services. I'm designed to help with:\n",
    "\n",
    "- **Policy Analysis:** Understanding city policies and regulations\n",
    "- **Citizen Reports:** Submitting and tracking municipal issues\n",
    "- **Environmental Tips:** Sustainable living recommendations\n",
    "- **Traffic & Routes:** Transportation planning and suggestions\n",
    "- **Data Analysis:** KPI monitoring and anomaly detection\n",
    "- **General Questions:** Smart city planning and services\n",
    "\n",
    "Please provide more specific details about what you'd like to know, and I'll be happy to assist you with relevant information and recommendations.\"\"\"\n",
    "\n",
    "    def policy_summarization(self, policy_text):\n",
    "        \"\"\"Summarize complex policy documents using IBM Granite\"\"\"\n",
    "        prompt = f\"\"\"Please analyze and summarize the following city policy document for citizens. Focus on:\n",
    "1. Main objectives and goals\n",
    "2. Key changes that affect residents\n",
    "3. Implementation timeline and important dates\n",
    "4. Citizen benefits and responsibilities\n",
    "\n",
    "Policy Document:\n",
    "{policy_text[:1500]}\n",
    "\n",
    "Provide a clear, citizen-friendly summary:\"\"\"\n",
    "        \n",
    "        return self.generate_response(prompt, max_tokens=400)\n",
    "\n",
    "    def process_citizen_feedback(self, report_data):\n",
    "        \"\"\"Process and categorize citizen feedback reports\"\"\"\n",
    "        categories = {\n",
    "            'water': ['water', 'pipe', 'leak', 'drainage', 'sewage', 'flooding'],\n",
    "            'traffic': ['traffic', 'road', 'signal', 'parking', 'accident', 'congestion'],\n",
    "            'environment': ['waste', 'pollution', 'noise', 'air', 'garbage', 'recycling'],\n",
    "            'infrastructure': ['street', 'light', 'sidewalk', 'building', 'construction', 'maintenance'],\n",
    "            'safety': ['crime', 'safety', 'police', 'emergency', 'security', 'lighting']\n",
    "        }\n",
    "\n",
    "        # Auto-categorize based on keywords\n",
    "        description = report_data.get('description', '').lower()\n",
    "        category = 'general'\n",
    "\n",
    "        for cat, keywords in categories.items():\n",
    "            if any(keyword in description for keyword in keywords):\n",
    "                category = cat\n",
    "                break\n",
    "\n",
    "        # Generate AI response using IBM Granite\n",
    "        prompt = f\"\"\"As a city administration representative, please provide a professional response to this citizen report:\n",
    "\n",
    "Issue: {report_data.get('description', '')}\n",
    "Location: {report_data.get('location', 'Location not specified')}\n",
    "\n",
    "Please provide:\n",
    "1. Acknowledgment of the report\n",
    "2. Immediate steps being taken\n",
    "3. Expected timeline for resolution\n",
    "4. Contact information for follow-up\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "        ai_response = self.generate_response(prompt, max_tokens=200)\n",
    "\n",
    "        # Store report\n",
    "        report = {\n",
    "            'id': len(self.citizen_reports) + 1,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'category': category,\n",
    "            'description': report_data.get('description'),\n",
    "            'location': report_data.get('location'),\n",
    "            'contact': report_data.get('contact'),\n",
    "            'priority': self.assess_priority(description),\n",
    "            'ai_response': ai_response,\n",
    "            'status': 'pending'\n",
    "        }\n",
    "\n",
    "        self.citizen_reports.append(report)\n",
    "        return report\n",
    "\n",
    "    def assess_priority(self, description):\n",
    "        \"\"\"Assess priority of citizen reports\"\"\"\n",
    "        high_priority_keywords = ['emergency', 'urgent', 'burst', 'fire', 'accident', 'danger', 'immediate']\n",
    "        medium_priority_keywords = ['broken', 'damaged', 'blocked', 'overflow', 'malfunction']\n",
    "\n",
    "        description_lower = description.lower()\n",
    "\n",
    "        if any(keyword in description_lower for keyword in high_priority_keywords):\n",
    "            return 'high'\n",
    "        elif any(keyword in description_lower for keyword in medium_priority_keywords):\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "    def kpi_forecasting(self, csv_data, kpi_type):\n",
    "        \"\"\"Forecast KPI values using machine learning\"\"\"\n",
    "        try:\n",
    "            # Parse CSV data\n",
    "            df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "            # Prepare data for forecasting\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df = df.sort_values('date')\n",
    "\n",
    "                # Create features\n",
    "                df['month'] = df['date'].dt.month\n",
    "                df['year'] = df['date'].dt.year\n",
    "                df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "                # Assume the last column is the target KPI\n",
    "                target_col = df.columns[-1]\n",
    "                feature_cols = ['month', 'year', 'day_of_year']\n",
    "\n",
    "                X = df[feature_cols].values\n",
    "                y = df[target_col].values\n",
    "\n",
    "                # Train simple linear regression\n",
    "                model = LinearRegression()\n",
    "                model.fit(X, y)\n",
    "\n",
    "                # Forecast next 12 months\n",
    "                last_date = df['date'].max()\n",
    "                forecasts = []\n",
    "\n",
    "                for i in range(1, 13):\n",
    "                    future_date = last_date + timedelta(days=30*i)\n",
    "                    features = [future_date.month, future_date.year, future_date.timetuple().tm_yday]\n",
    "                    prediction = model.predict([features])[0]\n",
    "\n",
    "                    forecasts.append({\n",
    "                        'date': future_date.strftime('%Y-%m-%d'),\n",
    "                        'predicted_value': round(prediction, 2),\n",
    "                        'kpi_type': kpi_type\n",
    "                    })\n",
    "\n",
    "                # Generate insights using IBM Granite\n",
    "                avg_historical = np.mean(y)\n",
    "                avg_forecast = np.mean([f['predicted_value'] for f in forecasts])\n",
    "                trend = \"increasing\" if avg_forecast > avg_historical else \"decreasing\"\n",
    "\n",
    "                prompt = f\"\"\"Analyze these {kpi_type} KPI forecasting results for city planning:\n",
    "\n",
    "Historical Data:\n",
    "- Average value: {avg_historical:.2f}\n",
    "- Data points: {len(y)}\n",
    "\n",
    "Forecast Results:\n",
    "- Predicted average: {avg_forecast:.2f}\n",
    "- Trend: {trend}\n",
    "- Forecast period: 12 months\n",
    "\n",
    "Please provide insights and recommendations for city administrators regarding:\n",
    "1. What this trend means for the city\n",
    "2. Potential actions to take\n",
    "3. Areas that need attention\n",
    "4. Long-term planning considerations\"\"\"\n",
    "\n",
    "                insights = self.generate_response(prompt, max_tokens=300)\n",
    "\n",
    "                return {\n",
    "                    'forecasts': forecasts,\n",
    "                    'insights': insights,\n",
    "                    'trend': trend,\n",
    "                    'accuracy_score': 'Based on historical data patterns'\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {'error': f'Error processing KPI data: {str(e)}'}\n",
    "\n",
    "    def generate_eco_tips(self, keywords):\n",
    "        \"\"\"Generate eco-friendly tips using IBM Granite\"\"\"\n",
    "        prompt = f\"\"\"As an environmental sustainability expert, generate 5 practical and actionable eco-friendly tips for city residents related to: {', '.join(keywords)}\n",
    "\n",
    "Requirements:\n",
    "- Tips should be specific and easy to implement\n",
    "- Include both individual and community-level actions\n",
    "- Focus on urban/city living context\n",
    "- Provide measurable impact where possible\n",
    "\n",
    "Please format as numbered list with clear explanations:\"\"\"\n",
    "\n",
    "        return self.generate_response(prompt, max_tokens=400)\n",
    "\n",
    "    def anomaly_detection(self, csv_data):\n",
    "        \"\"\"Detect anomalies in KPI data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "            # Assume last column is the KPI value\n",
    "            kpi_col = df.columns[-1]\n",
    "            values = df[kpi_col].values.reshape(-1, 1)\n",
    "\n",
    "            # Standardize data\n",
    "            scaler = StandardScaler()\n",
    "            values_scaled = scaler.fit_transform(values)\n",
    "\n",
    "            # Detect anomalies using Isolation Forest\n",
    "            detector = IsolationForest(contamination=0.1, random_state=42)\n",
    "            anomalies = detector.fit_predict(values_scaled)\n",
    "\n",
    "            # Identify anomalous records\n",
    "            anomaly_indices = np.where(anomalies == -1)[0]\n",
    "            anomaly_records = []\n",
    "\n",
    "            for idx in anomaly_indices:\n",
    "                record = df.iloc[idx].to_dict()\n",
    "                record['anomaly_score'] = abs(values_scaled[idx][0])\n",
    "                anomaly_records.append(record)\n",
    "\n",
    "            # Generate AI analysis using IBM Granite\n",
    "            if anomaly_records:\n",
    "                anomaly_values = [record[kpi_col] for record in anomaly_records]\n",
    "                prompt = f\"\"\"Analyze these anomalies detected in city monitoring data:\n",
    "\n",
    "Data Summary:\n",
    "- Total records analyzed: {len(df)}\n",
    "- Anomalies found: {len(anomaly_records)}\n",
    "- Anomalous values: {anomaly_values}\n",
    "- Normal range average: {np.mean(values):.2f}\n",
    "- Standard deviation: {np.std(values):.2f}\n",
    "\n",
    "Please provide analysis for city administrators:\n",
    "1. Possible causes for these anomalies\n",
    "2. Immediate actions to investigate\n",
    "3. Long-term monitoring recommendations\n",
    "4. Risk assessment and priorities\n",
    "\n",
    "Analysis:\"\"\"\n",
    "\n",
    "                analysis = self.generate_response(prompt, max_tokens=300)\n",
    "            else:\n",
    "                analysis = \"No significant anomalies detected in the provided data. The values appear to be within normal operational ranges.\"\n",
    "\n",
    "            return {\n",
    "                'anomalies_found': len(anomaly_records),\n",
    "                'anomaly_records': anomaly_records,\n",
    "                'analysis': analysis,\n",
    "                'total_records': len(df)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {'error': f'Error detecting anomalies: {str(e)}'}\n",
    "\n",
    "    def chat_assistant(self, message):\n",
    "        \"\"\"General chat assistant for city-related queries using IBM Granite\"\"\"\n",
    "        prompt = f\"\"\"You are a knowledgeable Smart City Assistant helping citizens with urban planning, sustainability, city services, and civic matters.\n",
    "\n",
    "Citizen Question: {message}\n",
    "\n",
    "Please provide a comprehensive, helpful, and practical answer that addresses their question. Include specific examples and actionable advice where appropriate.\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "        return self.generate_response(prompt, max_tokens=400)\n",
    "\n",
    "    def traffic_route_suggestion(self, origin, destination, city):\n",
    "        \"\"\"Generate traffic route suggestions using IBM Granite\"\"\"\n",
    "        prompt = f\"\"\"As a transportation planning expert, provide comprehensive travel advice for someone traveling from {origin} to {destination} in {city}.\n",
    "\n",
    "Please include:\n",
    "1. General route suggestions and main roads to consider\n",
    "2. Famous places and attractions near the destination\n",
    "3. Best times to travel to avoid traffic congestion\n",
    "4. Available transportation options (public transit, ride-sharing, etc.)\n",
    "5. Parking recommendations if driving\n",
    "6. Any special considerations for this route\n",
    "\n",
    "Travel Advisory:\"\"\"\n",
    "\n",
    "        return self.generate_response(prompt, max_tokens=400)\n",
    "\n",
    "print(\"ü§ñ SmartCityAssistant class defined with IBM Granite model!\")\n",
    "print(\"‚úÖ Configured for ibm-granite/granite-3.0-2b-instruct\")\n",
    "print(\"‚ö†Ô∏è Make sure to update your HuggingFace token for model access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03dcd3",
   "metadata": {},
   "source": [
    "## üöÄ Initialize Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694a898",
   "metadata": {},
   "source": [
    "## üîê IBM Granite Model Access\n",
    "\n",
    "**Important:** The IBM Granite model may require special access permissions.\n",
    "\n",
    "### If you get access errors:\n",
    "1. **Visit the model page:** https://huggingface.co/ibm-granite/granite-3.0-2b-instruct\n",
    "2. **Request access** if needed (click \"Request access\" button)\n",
    "3. **Wait for approval** (usually automatic for most users)\n",
    "4. **Use your HuggingFace token** in the configuration above\n",
    "\n",
    "### Alternative approach:\n",
    "If you have issues with model access, you can also try:\n",
    "```python\n",
    "# Login to HuggingFace (uncomment and run if needed)\n",
    "# !huggingface-cli login\n",
    "```\n",
    "\n",
    "### Model Information:\n",
    "- **Model:** IBM Granite 3.0 2B Instruct\n",
    "- **Size:** ~2 billion parameters\n",
    "- **Optimized for:** Instruction following and chat\n",
    "- **Requirements:** HuggingFace account and token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Smart City Assistant\n",
    "print(\"üöÄ Initializing Smart City Assistant...\")\n",
    "assistant = SmartCityAssistant(HF_TOKEN)\n",
    "print(\"‚úÖ Assistant initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b1ec8",
   "metadata": {},
   "source": [
    "## üé® Create Gradio Interface for Easy Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface for each functionality\n",
    "\n",
    "def policy_analysis_interface(policy_text):\n",
    "    \"\"\"Policy Analysis Interface\"\"\"\n",
    "    if not policy_text.strip():\n",
    "        return \"Please enter policy text to analyze.\"\n",
    "    return assistant.policy_summarization(policy_text)\n",
    "\n",
    "def citizen_report_interface(description, location, contact):\n",
    "    \"\"\"Citizen Report Interface\"\"\"\n",
    "    if not description.strip():\n",
    "        return \"Please provide a description of the issue.\"\n",
    "    \n",
    "    report_data = {\n",
    "        'description': description,\n",
    "        'location': location,\n",
    "        'contact': contact\n",
    "    }\n",
    "    \n",
    "    report = assistant.process_citizen_feedback(report_data)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    üìã **Report Submitted Successfully!**\n",
    "    \n",
    "    **Report ID:** {report['id']}\n",
    "    **Category:** {report['category'].title()}\n",
    "    **Priority:** {report['priority'].title()}\n",
    "    **Status:** {report['status'].title()}\n",
    "    \n",
    "    **AI Response:**\n",
    "    {report['ai_response']}\n",
    "    \"\"\"\n",
    "\n",
    "def kpi_forecast_interface(csv_data, kpi_type):\n",
    "    \"\"\"KPI Forecasting Interface\"\"\"\n",
    "    if not csv_data.strip():\n",
    "        return \"Please provide CSV data for forecasting.\"\n",
    "    \n",
    "    result = assistant.kpi_forecasting(csv_data, kpi_type)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        return f\"Error: {result['error']}\"\n",
    "    \n",
    "    forecast_text = \"\\n\".join([f\"üìÖ {f['date']}: {f['predicted_value']} ({f['kpi_type']})\" for f in result['forecasts'][:6]])\n",
    "    \n",
    "    return f\"\"\"\n",
    "    üìà **KPI Forecast Results**\n",
    "    \n",
    "    **Trend:** {result['trend'].title()}\n",
    "    \n",
    "    **Next 6 Months Forecast:**\n",
    "    {forecast_text}\n",
    "    \n",
    "    **AI Insights:**\n",
    "    {result['insights']}\n",
    "    \"\"\"\n",
    "\n",
    "def eco_tips_interface(keywords_text):\n",
    "    \"\"\"Eco Tips Interface\"\"\"\n",
    "    if not keywords_text.strip():\n",
    "        return \"Please provide keywords for eco tips (e.g., energy, water, transportation).\"\n",
    "    \n",
    "    keywords = [k.strip() for k in keywords_text.split(',')]\n",
    "    tips = assistant.generate_eco_tips(keywords)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    üå± **Eco-Friendly Tips**\n",
    "    \n",
    "    {tips}\n",
    "    \"\"\"\n",
    "\n",
    "def anomaly_detection_interface(csv_data):\n",
    "    \"\"\"Anomaly Detection Interface\"\"\"\n",
    "    if not csv_data.strip():\n",
    "        return \"Please provide CSV data for anomaly detection.\"\n",
    "    \n",
    "    result = assistant.anomaly_detection(csv_data)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        return f\"Error: {result['error']}\"\n",
    "    \n",
    "    return f\"\"\"\n",
    "    üîç **Anomaly Detection Results**\n",
    "    \n",
    "    **Total Records Analyzed:** {result['total_records']}\n",
    "    **Anomalies Found:** {result['anomalies_found']}\n",
    "    \n",
    "    **Analysis:**\n",
    "    {result['analysis']}\n",
    "    \"\"\"\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    \"\"\"Chat Interface\"\"\"\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "    \n",
    "    response = assistant.chat_assistant(message)\n",
    "    history.append([message, response])\n",
    "    \n",
    "    return history, \"\"\n",
    "\n",
    "def traffic_route_interface(origin, destination, city):\n",
    "    \"\"\"Traffic Route Interface\"\"\"\n",
    "    if not all([origin.strip(), destination.strip(), city.strip()]):\n",
    "        return \"Please provide origin, destination, and city information.\"\n",
    "    \n",
    "    suggestions = assistant.traffic_route_suggestion(origin, destination, city)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    üöó **Traffic Route Suggestions**\n",
    "    \n",
    "    **Route:** {origin} ‚Üí {destination} ({city})\n",
    "    \n",
    "    {suggestions}\n",
    "    \"\"\"\n",
    "\n",
    "print(\"üé® Gradio interface functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae4808",
   "metadata": {},
   "source": [
    "## üåê Launch Complete Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive Gradio interface with all functionalities\n",
    "\n",
    "with gr.Blocks(\n",
    "    title=\"üèôÔ∏è Sustainable Smart City Assistant\",\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "    .gradio-container {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "    }\n",
    "    .gr-button-primary {\n",
    "        background: linear-gradient(45deg, #667eea, #764ba2);\n",
    "        border: none;\n",
    "    }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # üèôÔ∏è Sustainable Smart City Assistant\n",
    "    \n",
    "    **Powered by IBM Granite 3.0 AI Model**\n",
    "    \n",
    "    Welcome to your comprehensive AI-powered smart city management platform!\n",
    "    \n",
    "    **Available Features:**\n",
    "    - üìã **Policy Analysis**: Summarize complex policy documents\n",
    "    - üì¢ **Citizen Reports**: Submit and manage citizen feedback\n",
    "    - üìà **KPI Forecasting**: Predict city metrics using ML\n",
    "    - üå± **Eco Tips**: Get sustainability recommendations\n",
    "    - üîç **Anomaly Detection**: Find unusual patterns in data\n",
    "    - üí¨ **AI Chat**: Ask questions about city management\n",
    "    - üöó **Traffic Routes**: Get route suggestions and travel tips\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # Policy Analysis Tab\n",
    "        with gr.TabItem(\"üìã Policy Analysis\"):\n",
    "            gr.Markdown(\"### Analyze and summarize city policy documents\")\n",
    "            gr.Markdown(\"*Powered by IBM Granite AI for accurate policy interpretation*\")\n",
    "            policy_input = gr.Textbox(\n",
    "                label=\"Policy Document Text\",\n",
    "                placeholder=\"Paste your policy document here...\",\n",
    "                lines=8\n",
    "            )\n",
    "            policy_button = gr.Button(\"Analyze Policy\", variant=\"primary\")\n",
    "            policy_output = gr.Textbox(label=\"Policy Summary\", lines=10)\n",
    "            \n",
    "            policy_button.click(\n",
    "                policy_analysis_interface,\n",
    "                inputs=[policy_input],\n",
    "                outputs=[policy_output]\n",
    "            )\n",
    "        \n",
    "        # Citizen Reports Tab\n",
    "        with gr.TabItem(\"üì¢ Citizen Reports\"):\n",
    "            gr.Markdown(\"### Submit citizen feedback and issues\")\n",
    "            gr.Markdown(\"*AI-powered categorization and automated responses*\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    report_desc = gr.Textbox(\n",
    "                        label=\"Issue Description\",\n",
    "                        placeholder=\"Describe the issue you want to report...\",\n",
    "                        lines=4\n",
    "                    )\n",
    "                    report_location = gr.Textbox(\n",
    "                        label=\"Location\",\n",
    "                        placeholder=\"Where is this issue located?\"\n",
    "                    )\n",
    "                    report_contact = gr.Textbox(\n",
    "                        label=\"Contact Information (Optional)\",\n",
    "                        placeholder=\"Your email or phone number\"\n",
    "                    )\n",
    "                    report_button = gr.Button(\"Submit Report\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    report_output = gr.Textbox(label=\"Report Status\", lines=12)\n",
    "            \n",
    "            report_button.click(\n",
    "                citizen_report_interface,\n",
    "                inputs=[report_desc, report_location, report_contact],\n",
    "                outputs=[report_output]\n",
    "            )\n",
    "        \n",
    "        # KPI Forecasting Tab\n",
    "        with gr.TabItem(\"üìà KPI Forecasting\"):\n",
    "            gr.Markdown(\"### Forecast city KPIs using machine learning\")\n",
    "            gr.Markdown(\"**Sample format:** `date,value` with rows like `2024-01-01,100`\")\n",
    "            gr.Markdown(\"*Combines ML forecasting with IBM Granite AI insights*\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    kpi_csv = gr.Textbox(\n",
    "                        label=\"CSV Data\",\n",
    "                        placeholder=\"date,air_quality_index\\n2024-01-01,45\\n2024-02-01,52\\n2024-03-01,48\\n2024-04-01,55\\n2024-05-01,41\",\n",
    "                        lines=8\n",
    "                    )\n",
    "                    kpi_type = gr.Textbox(\n",
    "                        label=\"KPI Type\",\n",
    "                        placeholder=\"e.g., Air Quality, Traffic Volume, Energy Consumption\",\n",
    "                        value=\"Air Quality\"\n",
    "                    )\n",
    "                    kpi_button = gr.Button(\"Generate Forecast\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    kpi_output = gr.Textbox(label=\"Forecast Results\", lines=12)\n",
    "            \n",
    "            kpi_button.click(\n",
    "                kpi_forecast_interface,\n",
    "                inputs=[kpi_csv, kpi_type],\n",
    "                outputs=[kpi_output]\n",
    "            )\n",
    "        \n",
    "        # Eco Tips Tab\n",
    "        with gr.TabItem(\"üå± Eco Tips\"):\n",
    "            gr.Markdown(\"### Get personalized sustainability recommendations\")\n",
    "            gr.Markdown(\"*IBM Granite AI generates context-aware environmental tips*\")\n",
    "            eco_keywords = gr.Textbox(\n",
    "                label=\"Keywords (comma-separated)\",\n",
    "                placeholder=\"energy, water, transportation, waste\",\n",
    "                value=\"energy, water\"\n",
    "            )\n",
    "            eco_button = gr.Button(\"Generate Eco Tips\", variant=\"primary\")\n",
    "            eco_output = gr.Textbox(label=\"Eco-Friendly Tips\", lines=10)\n",
    "            \n",
    "            eco_button.click(\n",
    "                eco_tips_interface,\n",
    "                inputs=[eco_keywords],\n",
    "                outputs=[eco_output]\n",
    "            )\n",
    "        \n",
    "        # Anomaly Detection Tab\n",
    "        with gr.TabItem(\"üîç Anomaly Detection\"):\n",
    "            gr.Markdown(\"### Detect unusual patterns in city data\")\n",
    "            gr.Markdown(\"**Sample format:** `date,metric_value` with numerical values\")\n",
    "            gr.Markdown(\"*ML anomaly detection with IBM Granite AI analysis*\")\n",
    "            anomaly_csv = gr.Textbox(\n",
    "                label=\"CSV Data\",\n",
    "                placeholder=\"date,metric_value\\n2024-01-01,100\\n2024-01-02,105\\n2024-01-03,98\\n2024-01-04,200\",\n",
    "                lines=8\n",
    "            )\n",
    "            anomaly_button = gr.Button(\"Detect Anomalies\", variant=\"primary\")\n",
    "            anomaly_output = gr.Textbox(label=\"Anomaly Detection Results\", lines=10)\n",
    "            \n",
    "            anomaly_button.click(\n",
    "                anomaly_detection_interface,\n",
    "                inputs=[anomaly_csv],\n",
    "                outputs=[anomaly_output]\n",
    "            )\n",
    "        \n",
    "        # Chat Assistant Tab\n",
    "        with gr.TabItem(\"üí¨ AI Chat\"):\n",
    "            gr.Markdown(\"### Ask questions about city management and planning\")\n",
    "            gr.Markdown(\"*Powered by IBM Granite 3.0 for intelligent conversations*\")\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Smart City Assistant (IBM Granite AI)\",\n",
    "                height=400\n",
    "            )\n",
    "            chat_input = gr.Textbox(\n",
    "                label=\"Your Message\",\n",
    "                placeholder=\"Ask me anything about smart cities, urban planning, or sustainability...\"\n",
    "            )\n",
    "            chat_button = gr.Button(\"Send Message\", variant=\"primary\")\n",
    "            \n",
    "            chat_button.click(\n",
    "                chat_interface,\n",
    "                inputs=[chat_input, chatbot],\n",
    "                outputs=[chatbot, chat_input]\n",
    "            )\n",
    "            \n",
    "            chat_input.submit(\n",
    "                chat_interface,\n",
    "                inputs=[chat_input, chatbot],\n",
    "                outputs=[chatbot, chat_input]\n",
    "            )\n",
    "        \n",
    "        # Traffic Routes Tab\n",
    "        with gr.TabItem(\"üöó Traffic Routes\"):\n",
    "            gr.Markdown(\"### Get traffic route suggestions and travel information\")\n",
    "            gr.Markdown(\"*IBM Granite AI provides comprehensive travel planning*\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    traffic_origin = gr.Textbox(\n",
    "                        label=\"Origin\",\n",
    "                        placeholder=\"Starting location\"\n",
    "                    )\n",
    "                    traffic_destination = gr.Textbox(\n",
    "                        label=\"Destination\",\n",
    "                        placeholder=\"Where are you going?\"\n",
    "                    )\n",
    "                    traffic_city = gr.Textbox(\n",
    "                        label=\"City\",\n",
    "                        placeholder=\"Which city?\"\n",
    "                    )\n",
    "                    traffic_button = gr.Button(\"Get Route Suggestions\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    traffic_output = gr.Textbox(label=\"Route Suggestions\", lines=12)\n",
    "            \n",
    "            traffic_button.click(\n",
    "                traffic_route_interface,\n",
    "                inputs=[traffic_origin, traffic_destination, traffic_city],\n",
    "                outputs=[traffic_output]\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    \n",
    "    ### üîß System Status\n",
    "    - ü§ñ **AI Model:** IBM Granite 3.0 2B Instruct\n",
    "    - ‚úÖ **All 7 core functionalities** available\n",
    "    - üîÑ **Fallback responses** if model unavailable\n",
    "    - ‚ö° **Real-time processing** enabled\n",
    "    - üåê **Public sharing** via ngrok (if configured)\n",
    "    \n",
    "    **Need help?** Use the AI Chat tab to ask questions about any feature!\n",
    "    \n",
    "    ### üìù Requirements\n",
    "    - HuggingFace token required for IBM Granite model\n",
    "    - Model will use fallback responses if authentication fails\n",
    "    - All functionalities work regardless of model status\n",
    "    \"\"\")\n",
    "\n",
    "# Configure ngrok for public access\n",
    "try:\n",
    "    if NGROK_TOKEN and NGROK_TOKEN != \"token\":\n",
    "        print(\"üåê Configuring ngrok for public access...\")\n",
    "        ngrok.set_auth_token(NGROK_TOKEN)\n",
    "        print(\"‚úÖ ngrok configured successfully!\")\n",
    "        \n",
    "        # Launch with public sharing\n",
    "        print(\"üöÄ Launching Smart City Assistant Interface with public access...\")\n",
    "        print(\"ü§ñ Powered by IBM Granite 3.0 AI Model\")\n",
    "        demo.launch(\n",
    "            share=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7860,\n",
    "            show_error=True,\n",
    "            quiet=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ngrok token not configured - launching locally only\")\n",
    "        print(\"üöÄ Launching Smart City Assistant Interface locally...\")\n",
    "        print(\"ü§ñ Powered by IBM Granite 3.0 AI Model\")\n",
    "        demo.launch(\n",
    "            share=False,\n",
    "            server_name=\"0.0.0.0\", \n",
    "            server_port=7860,\n",
    "            show_error=True,\n",
    "            quiet=False\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error launching interface: {e}\")\n",
    "    print(\"üîÑ Trying basic launch...\")\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e797926",
   "metadata": {},
   "source": [
    "## üß™ Test All Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all functionalities to ensure everything works\n",
    "print(\"üß™ Testing all functionalities...\\n\")\n",
    "\n",
    "try:\n",
    "    # Test 1: Policy Analysis\n",
    "    print(\"1. üìã Testing Policy Analysis...\")\n",
    "    test_policy = \"The city council has approved a new environmental policy to reduce carbon emissions by 30% over the next 5 years through renewable energy initiatives and improved public transportation.\"\n",
    "    policy_result = policy_analysis_interface(test_policy)\n",
    "    print(f\"‚úÖ Policy Analysis: Working correctly ({len(policy_result)} characters)\\n\")\n",
    "\n",
    "    # Test 2: Citizen Report\n",
    "    print(\"2. üì¢ Testing Citizen Reports...\")\n",
    "    citizen_result = citizen_report_interface(\"Broken streetlight on Main Street\", \"123 Main St\", \"citizen@email.com\")\n",
    "    print(f\"‚úÖ Citizen Report: Working correctly\\n\")\n",
    "\n",
    "    # Test 3: Eco Tips\n",
    "    print(\"3. üå± Testing Eco Tips...\")\n",
    "    eco_result = eco_tips_interface(\"energy, water\")\n",
    "    print(f\"‚úÖ Eco Tips: Working correctly ({len(eco_result)} characters)\\n\")\n",
    "\n",
    "    # Test 4: Chat\n",
    "    print(\"4. üí¨ Testing Chat Assistant...\")\n",
    "    chat_result = assistant.chat_assistant(\"What are the benefits of smart city technology?\")\n",
    "    print(f\"‚úÖ Chat Assistant: Working correctly ({len(chat_result)} characters)\\n\")\n",
    "\n",
    "    # Test 5: Traffic Routes\n",
    "    print(\"5. üöó Testing Traffic Routes...\")\n",
    "    traffic_result = traffic_route_interface(\"Airport\", \"Downtown\", \"New York\")\n",
    "    print(f\"‚úÖ Traffic Routes: Working correctly ({len(traffic_result)} characters)\\n\")\n",
    "\n",
    "    # Test 6: KPI Forecasting\n",
    "    print(\"6. üìà Testing KPI Forecasting...\")\n",
    "    test_csv = \"\"\"date,air_quality_index\n",
    "2024-01-01,45\n",
    "2024-02-01,52\n",
    "2024-03-01,48\n",
    "2024-04-01,55\n",
    "2024-05-01,41\"\"\"\n",
    "    kpi_result = kpi_forecast_interface(test_csv, \"Air Quality\")\n",
    "    print(f\"‚úÖ KPI Forecasting: Working correctly\\n\")\n",
    "\n",
    "    # Test 7: Anomaly Detection\n",
    "    print(\"7. üîç Testing Anomaly Detection...\")\n",
    "    anomaly_csv = \"\"\"date,metric_value\n",
    "2024-01-01,100\n",
    "2024-01-02,105\n",
    "2024-01-03,98\n",
    "2024-01-04,200\n",
    "2024-01-05,95\"\"\"\n",
    "    anomaly_result = anomaly_detection_interface(anomaly_csv)\n",
    "    print(f\"‚úÖ Anomaly Detection: Working correctly\\n\")\n",
    "\n",
    "    print(\"üéâ All functionality tests completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Some tests encountered issues: {e}\")\n",
    "    print(\"This is normal - the system will still work in the Gradio interface\")\n",
    "\n",
    "print(\"\\nüìù Sample CSV data formats:\")\n",
    "print(\"\\n**For KPI Forecasting:**\")\n",
    "print(\"date,air_quality_index\")\n",
    "print(\"2024-01-01,45\")\n",
    "print(\"2024-02-01,52\")\n",
    "print(\"2024-03-01,48\")\n",
    "\n",
    "print(\"\\n**For Anomaly Detection:**\")\n",
    "print(\"date,metric_value\")\n",
    "print(\"2024-01-01,100\")\n",
    "print(\"2024-01-02,105\")\n",
    "print(\"2024-01-03,200\")  # This would be an anomaly\n",
    "\n",
    "print(\"\\nüéØ Ready to use! All 7 functionalities are available in the Gradio interface above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66dded",
   "metadata": {},
   "source": [
    "## üìñ Usage Instructions\n",
    "\n",
    "### üîë Setup Steps:\n",
    "1. **Update API Keys**: In the Configuration cell above, replace `\"token\"` with your actual tokens:\n",
    "   - Get HuggingFace token: https://huggingface.co/settings/tokens\n",
    "   - Get ngrok token: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "2. **Run All Cells**: Execute all cells in order from top to bottom\n",
    "\n",
    "3. **Use the Interface**: The Gradio interface provides easy access to all 7 functionalities\n",
    "\n",
    "### üéØ Available Features:\n",
    "\n",
    "1. **üìã Policy Analysis**: Paste policy documents to get citizen-friendly summaries\n",
    "2. **üì¢ Citizen Reports**: Submit issues with automatic categorization and priority assessment\n",
    "3. **üìà KPI Forecasting**: Upload CSV data to predict future city metrics\n",
    "4. **üå± Eco Tips**: Get personalized sustainability recommendations\n",
    "5. **üîç Anomaly Detection**: Find unusual patterns in your city data\n",
    "6. **üí¨ AI Chat**: Ask questions about smart cities and urban planning\n",
    "7. **üöó Traffic Routes**: Get route suggestions and travel information\n",
    "\n",
    "### üìä Sample Data Formats:\n",
    "\n",
    "**For KPI Forecasting & Anomaly Detection:**\n",
    "```\n",
    "date,air_quality_index\n",
    "2024-01-01,45\n",
    "2024-02-01,52\n",
    "2024-03-01,48\n",
    "```\n",
    "\n",
    "### üåê Sharing Your Application:\n",
    "- If you configured ngrok, you'll get a public URL to share\n",
    "- The interface works on mobile and desktop\n",
    "- All processing happens in real-time\n",
    "\n",
    "### üîß Troubleshooting:\n",
    "- If model loading fails, the system automatically falls back to a smaller model\n",
    "- All functions include error handling and user-friendly messages\n",
    "- Use the test cell to verify all functionalities work correctly"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
